{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "\n",
    "@dataclass\n",
    "class TmlMetric:\n",
    "    gold_labels: Any\n",
    "    predictions: Any\n",
    "    function: Any = None\n",
    "    class_names: Any = None\n",
    "    weights: Any = None\n",
    "    \n",
    "@dataclass\n",
    "class TmlLoss:\n",
    "    loss: Any\n",
    "\n",
    "class TorchMetricLogger:\n",
    "    def __init__(self, log_function=None):\n",
    "        \"\"\"\n",
    "        log_function should accept a dictionary of values per epoch and log them somewhere\n",
    "        like to tensorboard, weights&biases or neptune\n",
    "        \"\"\"\n",
    "        \n",
    "        # defaultdict, that stores our metrics sorted by a key\n",
    "        self.history = defaultdict(list)\n",
    "        \n",
    "        # during a batch, we only get minibatch aka partial data\n",
    "        # we calculate the metric at batch end\n",
    "        self.partial = defaultdict(list)\n",
    "        self.log_function = log_function\n",
    "        \n",
    "    def _add(self, group_name, metric, partial=False):\n",
    "        if not partial:\n",
    "            self.history[group_name].append(metric)\n",
    "        else:\n",
    "            self.partial[group_name].append(metric)\n",
    "        \n",
    "    def calc_metric(self, group_name, metric:Metric, partial=False):\n",
    "        \"\"\"\n",
    "        calculates the metric from self.metric_function and stores it in the history\n",
    "        \"\"\"\n",
    "        # first add the mean total metric\n",
    "        metric_value = float(metric.function(metric.gold_labels, metric.predictions))\n",
    "        self._add(group_name, metric_value, partial)\n",
    "        \n",
    "        if metric.class_names is not None:\n",
    "            # then for each class add its metric as well\n",
    "            for index, class_name in enumerate(metric.class_names):\n",
    "                class_metric = float(metric_function(gold_label[:, index], prediction[:, index]))\n",
    "        \n",
    "                self._add(group_name + \"_\" + class_name, class_metric, partial)\n",
    "            \n",
    "    def add_loss(self, group_name, loss, partial=False):\n",
    "        # in case this is a torch tensor, recast as float\n",
    "        loss = float(loss)\n",
    "        self._add(group_name, loss, partial)\n",
    "\n",
    "    def __call__(self, partial=False, **label_prediction):\n",
    "        \"\"\"\n",
    "        takes an arbitrary amount of different \"keys\" and a tuple with (label, prediction)\n",
    "        if data is batched during training, call it with partial=True\n",
    "        \n",
    "        example metric(train=(10, 10), test=(9, 10))\n",
    "        \"\"\"\n",
    "        for group_name, bench in label_prediction.items():\n",
    "            if isinstance(bench, TmlLoss):\n",
    "                self.add_loss(group_name, bench.loss, partial)\n",
    "            \n",
    "            elif isinstance(bench, TmlMetric):\n",
    "                self.calc_metric(\n",
    "                    group_name, \n",
    "                    bench,\n",
    "                    partial\n",
    "                )   \n",
    "            else:\n",
    "                raise Exception(\"Input Error\", f\"please pass the data in the Dataclass Loss or Metric, you passed {type(bench)}.\")\n",
    "            \n",
    "    def batch_end(self):\n",
    "        for group_name, metric in self.partial.items():           \n",
    "            # calculate the mean per entry in the metric\n",
    "            metric = np.mean(metric, axis=0)\n",
    "\n",
    "            self.history[group_name].append(metric)\n",
    "        \n",
    "        self.partial = defaultdict(list)\n",
    "        \n",
    "        # log the metrics to w&b\n",
    "        if self.log_function is not None:\n",
    "            log_output = {name: metric[-1] for name, metric in self.history.items()}\n",
    "            self.log_function(log_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def binary_accuracy(label, prediction):\n",
    "    prediction = (prediction > 0.5).double()\n",
    "    tp = (label == prediction).double()\n",
    "    return torch.mean(tp)    \n",
    "\n",
    "labels = torch.ones((100, 4))\n",
    "predictions = torch.zeros((100, 4))\n",
    "\n",
    "tml = TorchMetricLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tml(meow=TmlMetric(labels, predictions, binary_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tml(morty=TmlLoss(loss=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {'meow': [0.0], 'morty': [0.1]})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tml.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
